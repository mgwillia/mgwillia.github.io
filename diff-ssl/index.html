<!DOCTYPE html>
<html>
<style>
    p {
        font-size: 15px;
    }
</style>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Diffusion for Recognition</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1.">


    <!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
    <link rel="icon" type="image/png" href="img/seal_icon.png">
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">
    <link rel="stylesheet" href="css/bootstrap.min.css">

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-110862391-1"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());

        gtag('config', 'UA-110862391-1');
    </script>

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>

    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                Diffusion Models Beat GANs on Image Classification<br />
                <small>
                    Preprint, superseded by <a href="https://mgwillia.github.io/diffssl">Do text-free diffusion models learn discriminative visual representations?</a>
                </small>
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://soumik-kanad.github.io/">
                            Soumik Mukhopadhyay*
                        </a>
                    </li>
                    <li>
                        <a href="https://mgwillia.github.io">
                            Matt Gwilliam*
                        </a>
                    </li>
                    <li>
                        <a href="https://scholar.google.com/citations?user=DJRhPVgAAAAJ&hl=en">
                            Vatsal Agarwal
                        </a>
                    </li>
                    <li>
                        <a href="https://namithap10.github.io/">
                            Namitha Padmanabhan
                        </a>
                    </li>
                    <li>
                        <a href="https://archana1998.github.io/">
                            Archana Swaminathan
                        </a>
                    </li>
                    <li>
                        <a href="https://tianyizhou.github.io/">
                            Tianyi Zhou
                        </a>
                    </li>
                    <li>
                        <a href="http://www.cs.umd.edu/~abhinav/">
                            Abhinav Shrivastava
                        </a>
                    </li>
                </ul>
                University of Maryland, College Park
            </div>
        </div>


        <div class="row">
            <div class="col-md-4 col-md-offset-4 text-center">
                <ul class="nav nav-pills nav-justified">
                    <li>
                        <a href="https://arxiv.org/abs/2307.08702">
                            <h4><strong>Paper</strong></h4>
                        </a>
                    </li>
                    <li>
                        <a href="https://github.com/soumik-kanad/diffssl/blob/main/scripts/linear.sh">
                            <h4><strong>Code (linear)</strong></h4>
                        </a> 
                    </li>
                    <li>
                        <a href="https://github.com/soumik-kanad/diffssl/blob/main/scripts/attention.sh">
                            <h4><strong>Code (attention)</strong></h4>
                        </a>
                    </li>
                </ul>
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                    While many unsupervised learning models focus on one family of tasks, either generative or
                    discriminative, we explore the possibility of a unified representation learner: a model which uses a
                    single pre-training stage to address both families of tasks simultaneously.
                    We identify diffusion models as a prime candidate.
                    Diffusion models have risen to prominence as a state-of-the-art method for image generation,
                    denoising, inpainting, super-resolution, manipulation, etc.
                    Such models involve training a U-Net to iteratively predict and remove noise, and the resulting
                    model can synthesize high fidelity, diverse, novel images.
                    The U-Net architecture, as a convolution-based architecture, generates a diverse set of feature
                    representations in the form of intermediate feature maps.
                    We present our findings that these embeddings are useful beyond the noise prediction task, as they
                    contain discriminative information and can also be leveraged for classification.
                    We explore optimal methods for extracting and using these embeddings for classification tasks,
                    demonstrating promising results on the ImageNet classification task.
                    We find that with careful feature selection and pooling, diffusion models outperform comparable
                    generative-discriminative methods such as BigBiGAN for classification tasks.
                    We investigate diffusion models in the transfer learning regime, examining their performance on
                    several fine-grained visual classification datasets.
                    We compare these embeddings to those generated by competing architectures and pre-trainings for
                    classification tasks.
                </p>
            </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Overview
                </h3>
                <center>
                    <image src="imgs/teaser_fig.png" width="720px"></image>
                </center> <br>
                <p class="text-justify">
                    As the title suggests, we show that diffusion models are better than GANs for both image generation
                    as well as image classification.
                    A few recent papers have started working towards unified image representation modeling, with better
                    results than BigBiGAN.
                    While current diffusion models fall short of current state-of-the-art, we hope that by showing their
                    current performance and promise, we bring much deserved attention to their potential as joint
                    generation-classification models.
                    To this end, we explore their performance for some classification tasks, and distill general
                    principles for using their features effectively in these settings.
                </p>

                <h3>
                    Diffusion models are better than GANs for unified representation learning.
                </h3>
                <center>
                    <image src="imgs/main_table.png" width="360px"></image>
                </center> <br>
                <p class="text-justify">
                    They beat GAN-based models such as BigBiGAN for both image generation and image classification.
                    This occurs even though the BigBiGAN was designed and trained with classification in mind, whereas
                    the diffusion model originally targetted only generative tasks.
                    Note that with careful pooling and feature selection, we obtain better results.
                    In fact, with a learned attention head to perform operations on frozen features, our result improves
                    substantially.
                    Together, these results demonstrate the promise of diffusion features for recognition tasks.
                </p>

                <h3>
                    Performance is heavily reliant on block number, time step, and feature pooling.
                </h3>
                <center>
                    <image src="imgs/bn_t_acc.png" width="720px"></image>
                </center> <br>
                <p class="text-justify">
                    Feature selection matters.
                    Diffusion, unlike typical systems, has an additional hyperparamter: time step.
                    We find that the features from time steps around the start and middle of the range tend to be best.
                    In terms of block number, the features at or just after the bottleneck are the most useful.
                    The same trend holds true for features - some pooling is better than both no pooling and pooling
                    everything.
                </p>

                <h3>
                    Diffusion features are useful for downstream recognition tasks.
                </h3>
                <center>
                    <image src="imgs/fgvc.png" width="720px"></image>
                </center> <br>
                <p class="text-justify">
                    The diffusion model we use was originally trained for generation on ImageNet.
                    Nevertheless, we find the frozen features are useful for classifcation on FGVC datasets.
                    While other self-supervised methods tend to generate better features, diffusion is somewhat
                    competitive on Aircraft.
                    We suggest that with more research and perhaps minor adjustments to the pre-training, diffusion
                    could close the gap for these tasks.
                </p>

                <h3>
                    Feature selection settings depend on the data.
                </h3>
                <center>
                    <image src="imgs/cub_grid_lines.png" width="720px"></image>
                </center> <br>
                <p class="text-justify">
                    Surprisingly, the same features that are best for ImageNet are not necessarily the best for FGVC
                    datasets.
                    Here, we explore settings for CUB.
                    We find that the features at the bottleneck block tend to be best, and that smaller pooling kernels
                    (larger feature maps) work better as well.
                    This introduces an additional difficulty for extracting features for diffusion- choosing the ideal
                    settings for each dataset.
                    However, it also points to an important direction for future research: automatic feature selection,
                    as well as cross-time and cross-block feature pooling and aggreation.
                </p>

                <h3>
                    Diffusion features vary dramatically depending on time and block.
                </h3>
                <center>
                    <image src="imgs/cka_all_aligned1.png" width="720px"></image>
                </center> <br>
                <p class="text-justify">
                    We compute and plot centered kernel alignment (CKA) to compare features from diffusion and other
                    models.
                    By comparing diffusion features at different blocks and time steps both to each other, as well as to
                    other models, we see interesting patterns emerge.
                    The similarities between diffusion and other models on the diagonal of each plot suggests that, like
                    other methods, diffusion models learn features that gradually become more discriminative toward
                    later layers.
                    Additionally, the near-zero values in, for example, the plot in the top right corner, show that
                    diffusion features at different blocks are more different from each other than they are compared to
                    features from other methods.
                    Overall, the qualitative trends in the CKA confirm our findings about the usefullness of the
                    features for different block numbers and time steps.
                </p>

            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly rows="8">
@misc{mukhopadhyay2023diffusion,
    title={Diffusion Models Beat GANs on Image Classification}, 
    author={Soumik Mukhopadhyay and Matthew Gwilliam and Vatsal Agarwal and Namitha Padmanabhan and Archana Swaminathan and Srinidhi Hegde and Tianyi Zhou and Abhinav Shrivastava},
    year={2023},
    eprint={2307.08702},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}
                    </textarea>
                </div>
            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <br> The website was based on the popular template from <a href="https://bmild.github.io">Ben
                    Mildenhall</a>.
                </p>
            </div>
        </div>
    </div>
</body>

</html>