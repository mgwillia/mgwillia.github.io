<!DOCTYPE HTML>
<html lang="en">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Matthew Gwilliam</title>

    <meta name="author" content="Matthew Gwilliam">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" type="image/png" href="images/umd_square.png">
</head>

<body>
    <table
        style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <tbody>
            <tr style="padding:0px">
                <td style="padding:0px">
                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr style="padding:0px">
                                <td style="padding:2.5%;width:63%;vertical-align:middle">
                                    <p style="text-align:center">
                                        <name>Matthew Gwilliam</name>
                                    </p>
                                    <p>I am a fifth year Ph.D student in the department of <a
                                            href="https://www.cs.umd.edu/">Computer Science</a> at the University of
                                        Maryland <a href="https://umd.edu/">(UMD)</a>, advised by <a
                                            href="https://www.cs.umd.edu/~abhinav/">Professor Abhinav Shrivastava</a>. I
                                        am studying computer vision.
                                    </p>
                                    <p>
                                        I completed my B.S. in Computer Science at <a
                                            href="https://www.byu.edu/">Brigham Young University</a>, where I was fortunate to work with <a
                                            href="https://faculty.cs.byu.edu/~farrell/">Professor Ryan Farrell</a>.
                                    </p>
                                    <p>
                                        During my PhD, I have enjoyed opportunities to work as an intern at Amazon, SRI, and NVIDIA.
                                    </p>
                                    <p style="text-align:center">
                                        <a href="mailto:mgwillia@cs.umd.edu">Email</a> &nbsp/&nbsp
                                        <a href="data/Resume.pdf">CV</a> &nbsp/&nbsp
                                        <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp -->
                                        <a href="https://scholar.google.com/citations?user=lB9WkQ0AAAAJ&hl=en">Google
                                            Scholar</a> <!-- &nbsp/&nbsp -->
                                        <!--<a href="https://twitter.com/">Twitter</a> -->
                                        <!-- <a href="https://github.com/jonbarron/">Github</a> -->
                                    </p>
                                </td>
                                <td style="padding:2.5%;width:40%;max-width:40%">
                                    <a href="images/matt_casual_head.jpg"><img style="width:100%;max-width:100%"
                                            alt="profile photo" src="images/matt_casual_head.jpg"
                                            class="hoverZoomLink"></a>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>
                            <tr>
                                <td style="padding:20px;width:100%;vertical-align:middle">
                                    <heading>Research</heading>
                                    <p>
                                        I am interested in computer vision models that learn without labels.
                                        More specifically, I am interested in methods that can learn universal image
                                        representations in an unsupervised manner.
                                        Currently, that work focuses on models based on diffusion and implicit neural
                                        representation (but mostly on INR).
                                        I am working with the sorts of tasks that these models are useful for: video
                                        retrieval, compression, generation; image classification, clustering, etc.
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                    <table
                        style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                        <tbody>

                            <tr>
                                <td style="padding:40px;width:25%;vertical-align:middle">
                                    <img src="ilf/imgs/ilf_method.png" alt="ilf" width="160"
                                        style="border-style: none">
                                </td>
                                <td width="75%" valign="middle">
                                    <papertitle>Accelerate High-Quality Diffusion Models with Inner Loop Feedback</papertitle>
                                    <br>
                                    <strong>Matthew Gwilliam*</strong>,
                                    <a href="https://han-cai.github.io/">Han Cai</a>,
                                    <a href="https://scholar.google.com/citations?user=rNBy1X4AAAAJ&hl=en">Di Wu</a>,
                                    <a href="https://www.cs.umd.edu/~abhinav/">Abhinav Shrivastava</a>
                                    <a href="https://scholar.google.com/citations?user=Jy6I9AIAAAAJ&hl=en">Zhiyu Cheng</a>,
                                    <br>
                                    <em> Submission Under Review </em>,
                                    2025
                                    <br>
                                    <a href="https://mgwillia.github.io/ilf/">Project Page</a> | <a
                                        href="https://arxiv.org/abs/2501.13107">Paper</a>
                                    <p>A novel lightweight feedback mechanism that creates a powerful inner loop within a diffusion model, allowing for more powerful diffusion denoising steps.
                                        We can use few powerful feedback steps to accomplish the job of many normal steps, allowing for higher image quailty in less inference time.
                                    </p>
                                </td>
                            </tr>
                            
                            <tr>
                                <td style="padding:40px;width:25%;vertical-align:middle">
                                    <img src="diffssl/imgs/diffssl_teaser.png" alt="diff_ssl" width="160"
                                        style="border-style: none">
                                </td>
                                <td width="75%" valign="middle">
                                    <papertitle>Do Text-free Diffusion Models Learn Discriminative Visual Representations?</papertitle>
                                    <br>
                                    <a href="https://soumik-kanad.github.io/">Soumik Mukhopadhyay*</a>,
                                    <strong>Matthew Gwilliam*</strong>,
                                    <span>Yosuke
                                        Yamaguchi<sup>&#10013;</sup></span>,
                                    <a href="https://scholar.google.com/citations?user=DJRhPVgAAAAJ&hl=en">Vatsal
                                        Agarwal<sup>&#10013;</sup></a>,
                                    <a href="https://namithap10.github.io/">Namitha Padmanabhan</a>,
                                    <a href="https://archana1998.github.io/">Archana Swaminathan</a>,
                                    <a href="https://tianyizhou.github.io/">Tianyi Zhou</a>,
                                    <a href="https://www.cs.umd.edu/~abhinav/">Abhinav Shrivastava</a>
                                    <br>
                                    <em> European Conference on Computer Vision (ECCV) </em>,
                                    2024
                                    <br>
                                    <a href="https://mgwillia.github.io/diffssl/">Project Page</a> | <a
                                        href="https://arxiv.org/abs/2311.17921">Paper</a> | <a
                                        href="https://github.com/soumik-kanad/diffssl">Code</a>
                                    <p>Explore diffusion models as unified unsupervised image representation learning
                                        models for many recognition tasks. Propose DifFormer and DifFeed, novel mechanisms for fusing diffusion features 
                                    for image classification.</p>
                                </td>
                            </tr>


                            <tr>
                                <td style="padding:40px;width:25%;vertical-align:middle">
                                    <img src="images/latent_inr_teaser.png" alt="diff_ssl" width="160"
                                        style="border-style: none">
                                </td>
                                <td width="75%" valign="middle">
                                    <papertitle>Latent-INR: A Flexible Framework for Implicit Representations of Videos with Discriminative Semantics</papertitle>
                                    <br>
                                    <a href="https://www.cs.umd.edu/~shishira/">Shishira R Maiya*</a>,
                                    <a href="https://learn2phoenix.github.io/">Anubhav Gupta*</a>,
                                    <strong>Matthew Gwilliam</strong>,
                                    <a href="https://maxlikelihood.ai/">Max Ehrlich</a>,
                                    <a href="https://www.cs.umd.edu/~abhinav/">Abhinav Shrivastava</a>
                                    <br>
                                    <em> European Conference on Computer Vision (ECCV) </em>,
                                    2024
                                    <br>
                                    <a href="https://www.cs.umd.edu/~shishira/latent_inr/latent_inr.html">Project Page</a> | <a
                                        href="https://arxiv.org/abs/2408.02672">Paper</a>
                                    <p>Develop implicit neural video models that perform well not only for compression, but also for retrieval, chat, and more.</p>
                                </td>
                            </tr>

                            
                            <tr>
                                <td style="padding:40px;width:25%;vertical-align:middle">
                                    <img src="images/xinc_teaser.png" alt="xinc_teaser" width="160"
                                        style="border-style: none">
                                </td>
                                <td width="75%" valign="middle">
                                    <papertitle>Explaining the Implicit Neural Canvas (XINC): Connecting Pixels to Neurons by Tracing their Contributions</papertitle>
                                    <br>
                                    <a href="https://namithap10.github.io/">Namitha Padmanabhan*</a>,
                                    <strong>Matthew Gwilliam*</strong>,
                                    <a href="https://www.cs.umd.edu/~pulkit/">Pulkit Kumar</a>,
                                    <a href="https://www.cs.umd.edu/~shishira/">Shishira Maiya</a>,
                                    <a href="https://maxlikelihood.ai/">Max Ehrlich</a>,
                                    <a href="https://www.cs.umd.edu/~abhinav/">Abhinav Shrivastava</a>
                                    <br>
                                    <em> IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) </em>,
                                    2024
                                    <br>
                                    <a href="https://namithap10.github.io/xinc/">Project Page</a> | <a
                                        href="https://arxiv.org/abs/2401.10217">Paper</a> | <a
                                        href="https://github.com/namithap10/xinc">Code</a> 
                                    <p>
                                        XINC dissects Implicit Neural Representation (INR) models to understand how neurons represent images and videos and to 
                                        reveal the inner workings of INRs.
                                    </p>
                                </td>
                            </tr>
                            
                            <tr>
                                <td style="padding:40px;width:25%;vertical-align:middle">
                                    <img src="images/elusive_images_teaser.png" alt="unsup_teaser" width="160"
                                        style="border-style: none">
                                </td>
                                <td width="75%" valign="middle">
                                    <papertitle>Elusive Images: Beyond Coarse Analysis for Fine-Grained Recognition</papertitle>
                                    <br>
                                    <a href="https://scholar.google.com/citations?user=UEKWqIoAAAAJ&hl=en">Connor Anderson</a>
                                    <strong>Matthew Gwilliam</strong>,
                                    <span>Evelyn Gaskin</span>
                                    <a href="https://faculty.cs.byu.edu/~farrell/">Ryan Farrell</a>
                                    <br>
                                    <em> IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) </em>,
                                    2024
                                    <br>
                                    <a href="https://openaccess.thecvf.com/content/WACV2024/html/Anderson_Elusive_Images_Beyond_Coarse_Analysis_for_Fine-Grained_Recognition_WACV_2024_paper.html">Paper</a>
                                    <p>
                                        Identify error types and image difficulty among state-of-the-art models for fine-grained visual categorization.
                                    </p>
                                </td>
                            </tr>
                            

                            <tr>
                                <td style="padding:40px;width:25%;vertical-align:middle">
                                    <img src="10k-words/imgs/10k_teaser.png" alt="diff_ssl" width="160"
                                        style="border-style: none">
                                </td>
                                <td width="75%" valign="middle">
                                    <papertitle>A Video is Worth 10,000 Words: Training and Benchmarking with Diverse Captions for Better Long Video Retrieval</papertitle>
                                    <br>
                                    <strong>Matthew Gwilliam</strong>,
                                    <a href="https://mcogswell.io/">Michael Cogswell</a>,
                                    <a href="https://scholar.google.com/citations?user=5tD6LNEAAAAJ&hl=en">Meng Ye</a>,
                                    <a href="https://www.ksikka.com/">Karan Sikka</a>,
                                    <a href="https://www.cs.umd.edu/~abhinav/">Abhinav Shrivastava</a>,
                                    <a href="https://sites.google.com/view/ajaydivakaran/home">Ajay Divakaran</a>
                                    <br>
                                    <em> Under Review </em>
                                    <br>
                                    <a href="https://mgwillia.github.io/10k-words/">Project Page</a> | <a
                                        href="https://arxiv.org/abs/2312.00115">Paper</a>
                                    <p>Propose an alternative to paragraphs for long video retrieval, specifically, the 10k Words problem: every video should be able to be matched 
                                        with any valid description, so we generate many descriptions for every video (for 3 long video datasets), evaluate accordingly, and introduce novel finetuning for better 
                                        performance.</p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:40px;width:25%;vertical-align:middle">
                                    <img src="diff-ssl/imgs/teaser_fig.png" alt="diff_ssl" width="160"
                                        style="border-style: none">
                                </td>
                                <td width="75%" valign="middle">
                                    <papertitle>Diffusion Models Beat GANs on Image Classification</papertitle>
                                    <br>
                                    <strong>Matthew Gwilliam*</strong>,
                                    <a href="https://soumik-kanad.github.io/">Soumik Mukhopadhyay*</a>,
                                    <a href="https://scholar.google.com/citations?user=DJRhPVgAAAAJ&hl=en">Vatsal
                                        Agarwal</a>,
                                    <a href="https://namithap10.github.io/">Namitha Padmanabhan</a>,
                                    <a href="https://archana1998.github.io/">Archana Swaminathan</a>,
                                    <a href="https://tianyizhou.github.io/">Tianyi Zhou</a>,
                                    <a href="https://www.cs.umd.edu/~abhinav/">Abhinav Shrivastava</a>
                                    <br>
                                    <em> preprint only </em>
                                    <br>
                                    <a href="https://mgwillia.github.io/diff-ssl/">Project Page</a> | <a
                                        href="https://arxiv.org/abs/2307.08702">Paper</a>
                                    <p>Show the potential of diffusion models as unified unsupervised image representation learners.</p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:40px;width:25%;vertical-align:middle">
                                    <img src="images/hnerv_teaser.png" alt="hnerv_teaser" width="160"
                                        style="border-style: none">
                                </td>
                                <td width="75%" valign="middle">
                                    <papertitle>HNeRV: A Hybrid Neural Representation for Videos</papertitle>
                                    <br>
                                    <a href="https://haochen-rye.github.io/">Hao Chen</a>,
                                    <strong>Matthew Gwilliam</strong>,
                                    <a href="https://sites.google.com/site/sernam">Ser-Nam Lim</a>,
                                    <a href="https://www.cs.umd.edu/~abhinav/">Abhinav Shrivastava</a>
                                    <br>
                                    <em> IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) </em>,
                                    2023
                                    <br>
                                    <a href="https://haochen-rye.github.io/HNeRV/">Project Page</a> | <a
                                        href="https://arxiv.org/abs/2304.02633">Paper</a> | <a
                                        href="https://github.com/haochen-rye/HNeRV">Code</a>
                                    <p>Combine the strengths of implicit (NeRV) and explicit (autoencoder)
                                        representation to create a hybrid neural
                                        representation for video with good properties for representation, compression,
                                        and editing.
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:40px;width:25%;vertical-align:middle">
                                    <img src="images/cnerv_teaser.jpg" alt="cnerv_teaser" width="160"
                                        style="border-style: none">
                                </td>
                                <td width="75%" valign="middle">
                                    <papertitle>CNeRV: Content-adaptive Neural Representation for Visual Data
                                    </papertitle>
                                    <br>
                                    <a href="https://haochen-rye.github.io/">Hao Chen</a>,
                                    <strong>Matthew Gwilliam</strong>,
                                    <a href="https://boheumd.github.io/">Bo He</a>,
                                    <a href="https://sites.google.com/site/sernam">Ser-Nam Lim</a>,
                                    <a href="https://www.cs.umd.edu/~abhinav/">Abhinav Shrivastava</a>
                                    <br>
                                    <em> British Machine Vision Conference (BMVC)</em>,
                                    2022 <strong>(ORAL)</strong>
                                    <br>
                                    <a href="https://haochen-rye.github.io/CNeRV/">Project Page</a> | <a
                                        href="http://arxiv.org/abs/2211.10421">Paper</a>
                                    <p>Make implicit video representation networks generalize to unseen data by
                                        swapping time embedding for content-aware embedding
                                        that is computed as a unique summary of each frame.
                                    </p>
                                </td>
                            </tr>

                            <tr>
                                <td style="padding:40px;width:25%;vertical-align:middle">
                                    <img src="exploring-unsupervised/imgs/teaser_star.png" alt="unsup_teaser" width="160"
                                        style="border-style: none">
                                </td>
                                <td width="75%" valign="middle">
                                    <papertitle>Beyond Supervised vs. Unsupervised: Representative Benchmarking and Analysis of Image Representation Learning</papertitle>
                                    <br>
                                    <strong>Matthew Gwilliam</strong>,
                                    <a href="https://www.cs.umd.edu/~abhinav/">Abhinav Shrivastava</a>
                                    <br>
                                    <em> IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) </em>,
                                    2022
                                    <br>
                                    <a href="https://mgwillia.github.io/exploring-unsupervised/">Project Page</a> | <a
                                        href="https://arxiv.org/abs/2206.08347">Paper</a> | <a
                                        href="https://github.com/mgwillia/unsupervised-analysis">Code</a>
                                    <p>
                                        Examine, compare, and contrast popular unsupervised image representation learning methods,
                                        showing that there are significant differences based on specific algorithm used,
                                        and "supervised vs. unsupervised" comparisons which neglect these differences 
                                        tend to over-generalize.
                                    </p>
                                </td>
                            </tr>
                            
                            <tr>
                                <td style="padding:40px;width:25%;vertical-align:middle">
                                    <img src="images/race_bias_teaser_new.png" alt="race_bias_teaser" width="160"
                                        style="border-style: none">
                                </td>
                                <td width="75%" valign="middle">
                                    <papertitle>Rethinking Common Assumptions to Mitigate Racial Bias in Face Recognition Datasets</papertitle>
                                    <br>
                                    <strong>Matthew Gwilliam</strong>,
                                    <a href="https://srihegde.github.io/">Srinidhi Hegde</a>
                                    <a href="https://www.semanticscholar.org/author/Lade-Tinubu/2142877926">Lade Tinubu</a>
                                    <a href="https://www.cs.umd.edu/~hanson/">Alex Hanson</a>
                                    <br>
                                    <em> IEEE/CVF International Conference on Computer Vision Workshops (ICCVW) </em>,
                                    2021
                                    <br>
                                    <a href="https://arxiv.org/abs/2109.03229">Paper</a> | <a
                                        href="https://github.com/j-alex-hanson/rethinking-race-face-datasets">Code</a>
                                    <p>
                                        Reveal the role of data in racial bias for facial recognition systems, and the flaws 
                                        underlying the assumption that balanced data results in fair performance.
                                    </p>
                                </td>
                            </tr>
                            
                            <tr>
                                <td style="padding:40px;width:25%;vertical-align:middle">
                                    <img src="images/fair_comparison.png" alt="unsup_teaser" width="160"
                                        style="border-style: none">
                                </td>
                                <td width="75%" valign="middle">
                                    <papertitle>Fair Comparison: Quantifying Variance in Results for Fine-grained Visual Categorization</papertitle>
                                    <br>
                                    <strong>Matthew Gwilliam</strong>,
                                    <a href="https://www.adamteuscher.com/">Adam Teuscher</a>
                                    <a href="https://scholar.google.com/citations?user=UEKWqIoAAAAJ&hl=en">Connor Anderson</a>
                                    <a href="https://faculty.cs.byu.edu/~farrell/">Ryan Farrell</a>
                                    <br>
                                    <em> IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) </em>,
                                    2021
                                    <br>
                                    <a href="https://arxiv.org/abs/2109.03156">Paper</a>
                                    <p>
                                        Uncover the large, often-ignored amount of variance in FGVC systems across training runs, 
                                        on the dataset level, but more
                                        particularly in terms of the classification performance for individual classes.
                                    </p>
                                </td>
                            </tr>
                            
                            <tr>
                                <td style="padding:40px;width:25%;vertical-align:middle">
                                    <img src="images/intell_image.png" alt="unsup_teaser" width="160"
                                        style="border-style: none">
                                </td>
                                <td width="75%" valign="middle">
                                    <papertitle>Intelligent Image Collection: Building the Optimal Dataset</papertitle>
                                    <br>
                                    <strong>Matthew Gwilliam</strong>,
                                    <a href="https://faculty.cs.byu.edu/~farrell/">Ryan Farrell</a>
                                    <br>
                                    <em> IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) </em>,
                                    2020
                                    <br>
                                    <a href="https://openaccess.thecvf.com/content_WACV_2020/html/Gwilliam_Intelligent_Image_Collection_Building_the_Optimal_Dataset_WACV_2020_paper.html">Paper</a>
                                    <p>
                                        Propose smart practices to optimize image curation,
                                        such that classification accuracy is maximized
                                        for a given constrained dataset size.
                                    </p>
                                </td>
                            </tr>

                        </tbody>
                    </table>

                    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                        <tbody>
                            <tr>


                                <!-- <td> -->


                                <!-- <heading>Service</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpg"></td>
            <td width="75%" valign="center">
              <a href="http://cvpr2021.thecvf.com/area-chairs">Area Chair, CVPR 2021</a>
              <br><br>
              <a href="http://cvpr2019.thecvf.com/area_chairs">Area Chair, CVPR 2019</a>
              <br><br>
              <a href="http://cvpr2018.thecvf.com/organizers/area_chairs">Area Chair, CVPR 2018</a>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/cs188.jpg" alt="cs188">
            </td>
            <td width="75%" valign="center">
              <a href="http://inst.eecs.berkeley.edu/~cs188/sp11/announcements.html">Graduate Student Instructor, CS188 Spring 2011</a>
              <br>
              <br>
              <a href="http://inst.eecs.berkeley.edu/~cs188/fa10/announcements.html">Graduate Student Instructor, CS188 Fall 2010</a>
              <br>
              <br>
              <a href="http://aima.cs.berkeley.edu/">Figures, "Artificial Intelligence: A Modern Approach", 3rd Edition</a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> -->
                            <tr>
                                <td style="padding:0px">
                                    <br>
                                    <p style="text-align:right;font-size:small;">
                                        <a href="https://jonbarron.info/">Template credits</a>
                                    </p>
                                </td>
                            </tr>
                        </tbody>
                    </table>
                </td>
            </tr>
    </table>
</body>

</html>